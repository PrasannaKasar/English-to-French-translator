{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1067156,"sourceType":"datasetVersion","datasetId":592212}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y torch torchvision torchaudio\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OYbqYbHamLJh","outputId":"26818e57-e146-43cf-82e8-64f9f6720251","execution":{"iopub.status.busy":"2024-09-02T18:53:41.233630Z","iopub.execute_input":"2024-09-02T18:53:41.233924Z","iopub.status.idle":"2024-09-02T18:54:06.330102Z","shell.execute_reply.started":"2024-09-02T18:53:41.233891Z","shell.execute_reply":"2024-09-02T18:54:06.329139Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.4.0\nUninstalling torch-2.4.0:\n  Successfully uninstalled torch-2.4.0\nFound existing installation: torchvision 0.19.0\nUninstalling torchvision-0.19.0:\n  Successfully uninstalled torchvision-0.19.0\nFound existing installation: torchaudio 2.4.0\nUninstalling torchaudio-2.4.0:\n  Successfully uninstalled torchaudio-2.4.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install torch==2.2.0\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JXA0QkQGmPB5","outputId":"45934523-1cbb-40dd-86e6-69bc13e9ae5a","execution":{"iopub.status.busy":"2024-09-02T18:54:06.332082Z","iopub.execute_input":"2024-09-02T18:54:06.332453Z","iopub.status.idle":"2024-09-02T18:56:13.038012Z","shell.execute_reply.started":"2024-09-02T18:54:06.332417Z","shell.execute_reply":"2024-09-02T18:56:13.036891Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting torch==2.2.0\n  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0) (2024.6.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0)\n  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.2.0 (from torch==2.2.0)\n  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0)\n  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.0) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.0) (1.3.0)\nDownloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\neasyocr 1.7.1 requires torchvision>=0.5, which is not installed.\nfastai 2.7.16 requires torchvision>=0.11, which is not installed.\ntimm 1.0.8 requires torchvision, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 torch-2.2.0 triton-2.2.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install torchtext==0.17.0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bgYE8kCdmvFB","outputId":"9c2b9e08-c892-41c7-fe0f-a7af2deaf5db","execution":{"iopub.status.busy":"2024-09-02T18:56:13.039570Z","iopub.execute_input":"2024-09-02T18:56:13.039953Z","iopub.status.idle":"2024-09-02T18:56:32.776422Z","shell.execute_reply.started":"2024-09-02T18:56:13.039908Z","shell.execute_reply":"2024-09-02T18:56:32.775403Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting torchtext==0.17.0\n  Downloading torchtext-0.17.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.6 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torchtext==0.17.0) (4.66.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchtext==0.17.0) (2.32.3)\nRequirement already satisfied: torch==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torchtext==0.17.0) (2.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchtext==0.17.0) (1.26.4)\nCollecting torchdata==0.7.1 (from torchtext==0.17.0)\n  Downloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchtext==0.17.0) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchtext==0.17.0) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchtext==0.17.0) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchtext==0.17.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchtext==0.17.0) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchtext==0.17.0) (2024.6.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchtext==0.17.0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchtext==0.17.0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchtext==0.17.0) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchtext==0.17.0) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchtext==0.17.0) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchtext==0.17.0) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchtext==0.17.0) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchtext==0.17.0) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchtext==0.17.0) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchtext==0.17.0) (2.19.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchtext==0.17.0) (12.1.105)\nRequirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->torchtext==0.17.0) (2.2.0)\nRequirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.10/site-packages (from torchdata==0.7.1->torchtext==0.17.0) (1.26.18)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->torchtext==0.17.0) (12.6.68)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.17.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.17.0) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.17.0) (2024.7.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.0->torchtext==0.17.0) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.0->torchtext==0.17.0) (1.3.0)\nDownloading torchtext-0.17.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torchdata, torchtext\nSuccessfully installed torchdata-0.7.1 torchtext-0.17.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":304},"id":"mnGfqat5epvF","outputId":"05c139d0-845d-44bd-f2ab-1a0a064b2a57","execution":{"iopub.status.busy":"2024-09-02T18:56:32.778882Z","iopub.execute_input":"2024-09-02T18:56:32.779274Z","iopub.status.idle":"2024-09-02T18:56:32.783546Z","shell.execute_reply.started":"2024-09-02T18:56:32.779239Z","shell.execute_reply":"2024-09-02T18:56:32.782671Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nimport pandas as pd\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"id":"n3S-z5AxeD0a","execution":{"iopub.status.busy":"2024-09-02T18:56:32.784654Z","iopub.execute_input":"2024-09-02T18:56:32.784921Z","iopub.status.idle":"2024-09-02T18:56:35.087350Z","shell.execute_reply.started":"2024-09-02T18:56:32.784891Z","shell.execute_reply":"2024-09-02T18:56:35.086361Z"},"trusted":true},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/language-translation-englishfrench/eng_-french.csv')\ndf.columns = [\"eng\",\"fre\"]\ndf.head()","metadata":{"id":"PNsc7fbNeD0d","execution":{"iopub.status.busy":"2024-09-02T18:56:35.088650Z","iopub.execute_input":"2024-09-02T18:56:35.089455Z","iopub.status.idle":"2024-09-02T18:56:35.588881Z","shell.execute_reply.started":"2024-09-02T18:56:35.089410Z","shell.execute_reply":"2024-09-02T18:56:35.587870Z"},"trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"    eng         fre\n0   Hi.      Salut!\n1  Run!     Cours !\n2  Run!    Courez !\n3  Who?       Qui ?\n4  Wow!  Ça alors !","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eng</th>\n      <th>fre</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hi.</td>\n      <td>Salut!</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Run!</td>\n      <td>Cours !</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Run!</td>\n      <td>Courez !</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Who?</td>\n      <td>Qui ?</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Wow!</td>\n      <td>Ça alors !</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"print(df.isna().sum())\nprint(df.isnull().sum())","metadata":{"id":"wzOoUSbneD0e","execution":{"iopub.status.busy":"2024-09-02T18:56:35.590230Z","iopub.execute_input":"2024-09-02T18:56:35.590579Z","iopub.status.idle":"2024-09-02T18:56:35.670254Z","shell.execute_reply.started":"2024-09-02T18:56:35.590542Z","shell.execute_reply":"2024-09-02T18:56:35.669298Z"},"trusted":true},"outputs":[{"name":"stdout","text":"eng    0\nfre    0\ndtype: int64\neng    0\nfre    0\ndtype: int64\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# tokenize","metadata":{"id":"BKDtGYOLeD0e","execution":{"iopub.status.busy":"2024-09-02T18:56:35.671620Z","iopub.execute_input":"2024-09-02T18:56:35.671988Z","iopub.status.idle":"2024-09-02T18:56:35.677021Z","shell.execute_reply.started":"2024-09-02T18:56:35.671946Z","shell.execute_reply":"2024-09-02T18:56:35.675915Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import string\nimport re\n\nvalid_puncs = \"-,'!?.\"\ninvalid_puncs = string.punctuation         #returns all sets of punctuations\n\nfor punc in valid_puncs:\n    invalid_puncs = invalid_puncs.replace(punc,\"\")\ninvalid_puncs_re = re.compile(\"[\"+ invalid_puncs +\"]\")   #expression to match any invalid punctuations\n\ndef text_sanitization(text: str) -> str:\n    text = invalid_puncs_re.sub(\"\", text)\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text","metadata":{"id":"foXCDAUieD0f","execution":{"iopub.status.busy":"2024-09-02T18:56:35.678252Z","iopub.execute_input":"2024-09-02T18:56:35.678586Z","iopub.status.idle":"2024-09-02T18:56:35.687371Z","shell.execute_reply.started":"2024-09-02T18:56:35.678551Z","shell.execute_reply":"2024-09-02T18:56:35.686404Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"df[\"eng\"] = df[\"eng\"].apply(text_sanitization)\ndf[\"fre\"] = df[\"fre\"].apply(text_sanitization)\ndf.tail(5)","metadata":{"id":"BjcBzNfNeD0f","execution":{"iopub.status.busy":"2024-09-02T18:56:35.691671Z","iopub.execute_input":"2024-09-02T18:56:35.692046Z","iopub.status.idle":"2024-09-02T18:56:37.751245Z","shell.execute_reply.started":"2024-09-02T18:56:35.692012Z","shell.execute_reply":"2024-09-02T18:56:37.750250Z"},"trusted":true},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                                      eng  \\\n175616  Top-down economics never works, said Obama. Th...   \n175617  A carbon footprint is the amount of carbon dio...   \n175618  Death is something that we're often discourage...   \n175619  Since there are usually multiple websites on a...   \n175620  If someone who doesn't know your background sa...   \n\n                                                      fre  \n175616  « L'économie en partant du haut vers le bas, ç...  \n175617  Une empreinte carbone est la somme de pollutio...  \n175618  La mort est une chose qu'on nous décourage sou...  \n175619  Puisqu'il y a de multiples sites web sur chaqu...  \n175620  Si quelqu'un qui ne connaît pas vos antécédent...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eng</th>\n      <th>fre</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>175616</th>\n      <td>Top-down economics never works, said Obama. Th...</td>\n      <td>« L'économie en partant du haut vers le bas, ç...</td>\n    </tr>\n    <tr>\n      <th>175617</th>\n      <td>A carbon footprint is the amount of carbon dio...</td>\n      <td>Une empreinte carbone est la somme de pollutio...</td>\n    </tr>\n    <tr>\n      <th>175618</th>\n      <td>Death is something that we're often discourage...</td>\n      <td>La mort est une chose qu'on nous décourage sou...</td>\n    </tr>\n    <tr>\n      <th>175619</th>\n      <td>Since there are usually multiple websites on a...</td>\n      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n    </tr>\n    <tr>\n      <th>175620</th>\n      <td>If someone who doesn't know your background sa...</td>\n      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import math\nimport random\nimport re\nimport string\nfrom collections import Counter, OrderedDict\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset\nimport os","metadata":{"id":"GiCmDjmweD0g","execution":{"iopub.status.busy":"2024-09-02T18:56:37.752558Z","iopub.execute_input":"2024-09-02T18:56:37.752917Z","iopub.status.idle":"2024-09-02T18:56:39.358857Z","shell.execute_reply.started":"2024-09-02T18:56:37.752881Z","shell.execute_reply":"2024-09-02T18:56:39.357925Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"en_words = Counter()\nfor sentence in tqdm(df[\"eng\"]):\n    en_words.update(word_tokenize(sentence, language='english'))\nen_words = OrderedDict(sorted(en_words.items(), key=lambda x:-x[1]))\n\nfr_words = Counter()\nfor sentence in tqdm(df[\"fre\"]):\n    fr_words.update(word_tokenize(sentence, language='french'))\nfr_words = OrderedDict(sorted(fr_words.items(), key=lambda x:-x[1]))\n\nprint(f\"no. of english words: {len(en_words)}\")\nprint(f\"no. of french words: {len(fr_words)}\")","metadata":{"id":"n1Ta5adAeD0g","execution":{"iopub.status.busy":"2024-09-02T18:56:39.359980Z","iopub.execute_input":"2024-09-02T18:56:39.360442Z","iopub.status.idle":"2024-09-02T18:57:31.225098Z","shell.execute_reply.started":"2024-09-02T18:56:39.360407Z","shell.execute_reply":"2024-09-02T18:57:31.224141Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/175621 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c01557626c244ce488bad21d84ea628c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/175621 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d27ba3be0ac4936bb03f67a01ed51c8"}},"metadata":{}},{"name":"stdout","text":"no. of english words: 16220\nno. of french words: 31581\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"en_words['a']","metadata":{"execution":{"iopub.status.busy":"2024-09-02T19:09:32.130448Z","iopub.execute_input":"2024-09-02T19:09:32.130846Z","iopub.status.idle":"2024-09-02T19:09:32.137391Z","shell.execute_reply.started":"2024-09-02T19:09:32.130808Z","shell.execute_reply":"2024-09-02T19:09:32.136463Z"},"trusted":true},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"24320"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"from sys import version\nimport torch\ntorch.__version__","metadata":{"id":"ZpDk5gADhPHN","execution":{"iopub.status.busy":"2024-09-02T18:57:31.226601Z","iopub.execute_input":"2024-09-02T18:57:31.227037Z","iopub.status.idle":"2024-09-02T18:57:31.235955Z","shell.execute_reply.started":"2024-09-02T18:57:31.226990Z","shell.execute_reply":"2024-09-02T18:57:31.234975Z"},"trusted":true},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'2.2.0+cu121'"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"import torchtext\nfrom torchtext.vocab import vocab\n","metadata":{"id":"LRjPYAqeeD0i","execution":{"iopub.status.busy":"2024-09-02T18:57:31.237268Z","iopub.execute_input":"2024-09-02T18:57:31.237649Z","iopub.status.idle":"2024-09-02T18:57:32.631355Z","shell.execute_reply.started":"2024-09-02T18:57:31.237605Z","shell.execute_reply":"2024-09-02T18:57:32.630477Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"unk_token = '<unk>'  # Unknown\nsos_token = '<sos>'  # Start of sentence\neos_token = '<eos>'  # End of sentence\npad_token = '<pad>'  # Padding token\nen_vocab = torchtext.vocab.vocab(en_words, specials=[unk_token, eos_token, pad_token])\nen_vocab.set_default_index(en_vocab[unk_token])\nfr_vocab = torchtext.vocab.vocab(fr_words, specials=[unk_token, eos_token, sos_token, pad_token])\nfr_vocab.set_default_index(fr_vocab[unk_token])","metadata":{"id":"JTSCBuOyeD0j","execution":{"iopub.status.busy":"2024-09-02T18:57:32.632518Z","iopub.execute_input":"2024-09-02T18:57:32.632996Z","iopub.status.idle":"2024-09-02T18:57:32.833136Z","shell.execute_reply.started":"2024-09-02T18:57:32.632962Z","shell.execute_reply":"2024-09-02T18:57:32.832217Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"len(en_vocab), len(fr_vocab)","metadata":{"id":"Q6a0sdx2Xw2V","execution":{"iopub.status.busy":"2024-09-02T18:57:32.834594Z","iopub.execute_input":"2024-09-02T18:57:32.834923Z","iopub.status.idle":"2024-09-02T18:57:32.849082Z","shell.execute_reply.started":"2024-09-02T18:57:32.834889Z","shell.execute_reply":"2024-09-02T18:57:32.848003Z"},"trusted":true},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(16223, 31585)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"def en_tokenize(text: str, append_eos=True):\n    words = word_tokenize(text, language='english')\n    if append_eos:\n        words.append(eos_token)\n    return en_vocab(words)\n\ndef fr_tokenize(text: str, append_eos=True):\n    words = [sos_token] + word_tokenize(text, language='french')\n    if append_eos:\n        words.append(eos_token)\n    return fr_vocab(words)","metadata":{"id":"W3mQQmbceD0j","execution":{"iopub.status.busy":"2024-09-02T18:57:32.850446Z","iopub.execute_input":"2024-09-02T18:57:32.850916Z","iopub.status.idle":"2024-09-02T18:57:32.859051Z","shell.execute_reply.started":"2024-09-02T18:57:32.850863Z","shell.execute_reply":"2024-09-02T18:57:32.858184Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-09-02T19:04:26.436443Z","iopub.execute_input":"2024-09-02T19:04:26.436828Z","iopub.status.idle":"2024-09-02T19:04:26.478607Z","shell.execute_reply.started":"2024-09-02T19:04:26.436789Z","shell.execute_reply":"2024-09-02T19:04:26.477137Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m en_vocab:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchtext/vocab/vocab.py:65\u001b[0m, in \u001b[0;36mVocab.__getitem__\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mexport\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, token: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m        token: The token used to lookup the corresponding index.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m        The index corresponding to the associated token.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m\n","\u001b[0;31mTypeError\u001b[0m: __getitem__(): incompatible function arguments. The following argument types are supported:\n    1. (self: torchtext._torchtext.Vocab, arg0: str) -> int\n\nInvoked with: <torchtext._torchtext.Vocab object at 0x7c90a87f5a70>, 0"],"ename":"TypeError","evalue":"__getitem__(): incompatible function arguments. The following argument types are supported:\n    1. (self: torchtext._torchtext.Vocab, arg0: str) -> int\n\nInvoked with: <torchtext._torchtext.Vocab object at 0x7c90a87f5a70>, 0","output_type":"error"}],"execution_count":35},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(df['eng'], df['fre'], test_size=0.2)","metadata":{"id":"6Y_AJDVheD0k","execution":{"iopub.status.busy":"2024-09-02T18:57:32.871286Z","iopub.execute_input":"2024-09-02T18:57:32.871598Z","iopub.status.idle":"2024-09-02T18:57:32.924173Z","shell.execute_reply.started":"2024-09-02T18:57:32.871563Z","shell.execute_reply":"2024-09-02T18:57:32.923119Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"x_train_seqs = [en_tokenize(x) for x in x_train]\nx_test_seqs = [en_tokenize(x) for x in x_test]\n\ny_train_seqs = [fr_tokenize(y) for y in y_train]\ny_test_seqs = [fr_tokenize(y) for y in y_test]","metadata":{"id":"QzZnVAdIvN71","execution":{"iopub.status.busy":"2024-09-02T18:57:32.925546Z","iopub.execute_input":"2024-09-02T18:57:32.925877Z","iopub.status.idle":"2024-09-02T18:57:42.473719Z","shell.execute_reply.started":"2024-09-02T18:57:32.925840Z","shell.execute_reply":"2024-09-02T18:57:42.471370Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x_train_seqs \u001b[38;5;241m=\u001b[39m [en_tokenize(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x_train]\n\u001b[1;32m      2\u001b[0m x_test_seqs \u001b[38;5;241m=\u001b[39m [en_tokenize(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x_test]\n\u001b[1;32m      4\u001b[0m y_train_seqs \u001b[38;5;241m=\u001b[39m [fr_tokenize(y) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m y_train]\n","Cell \u001b[0;32mIn[20], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x_train_seqs \u001b[38;5;241m=\u001b[39m [\u001b[43men_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x_train]\n\u001b[1;32m      2\u001b[0m x_test_seqs \u001b[38;5;241m=\u001b[39m [en_tokenize(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m x_test]\n\u001b[1;32m      4\u001b[0m y_train_seqs \u001b[38;5;241m=\u001b[39m [fr_tokenize(y) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m y_train]\n","Cell \u001b[0;32mIn[17], line 2\u001b[0m, in \u001b[0;36men_tokenize\u001b[0;34m(text, append_eos)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21men_tokenize\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m, append_eos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m----> 2\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menglish\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m append_eos:\n\u001b[1;32m      4\u001b[0m         words\u001b[38;5;241m.\u001b[39mappend(eos_token)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/tokenize/__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    :type preserver_line: bool\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences\n\u001b[1;32m    132\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/tokenize/__init__.py:97\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(language))\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1235\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1283\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspan_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1266\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1250\u001b[0m             tokens\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1251\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(period_index\u001b[38;5;241m=\u001b[39mmatch\u001b[38;5;241m.\u001b[39mend() \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1252\u001b[0m             text\u001b[38;5;241m=\u001b[39mdecision_text,\n\u001b[1;32m   1253\u001b[0m             type1\u001b[38;5;241m=\u001b[39mtokens[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtype,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1263\u001b[0m             break_decision\u001b[38;5;241m=\u001b[39mtokens[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msentbreak,\n\u001b[1;32m   1264\u001b[0m         )\n\u001b[0;32m-> 1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspan_tokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the (start, end) spans of sentences\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;124;03m    in the text.\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1271\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slices_from_text(text)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":20},{"cell_type":"code","source":"class TranslationDataset(Dataset):\n    def __init__(self, src_sequences, tgt_sequences):\n        self.src_sequences = src_sequences\n        self.tgt_sequences = tgt_sequences\n\n    def __len__(self):\n        return len(self.src_sequences)\n\n    def __getitem__(self, idx):\n        src = self.src_sequences[idx]\n        tgt = self.tgt_sequences[idx]\n        return src, tgt","metadata":{"id":"vR0jvRKarreG","execution":{"iopub.status.busy":"2024-09-02T18:57:42.476900Z","iopub.status.idle":"2024-09-02T18:57:42.477744Z","shell.execute_reply.started":"2024-09-02T18:57:42.477444Z","shell.execute_reply":"2024-09-02T18:57:42.477476Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def pad_collate_fn(batch):\n    src_batch, tgt_batch = zip(*batch)\n\n    # Pad sequences\n    src_padded = pad_sequence([torch.tensor(seq) for seq in src_batch], batch_first=True, padding_value=0)\n    tgt_padded = pad_sequence([torch.tensor(seq) for seq in tgt_batch], batch_first=True, padding_value=0)\n\n    # Create padding masks\n    src_mask = (src_padded == 0).float()\n    tgt_mask = (tgt_padded == 0).float()\n\n    return src_padded, tgt_padded, src_mask, tgt_mask\n","metadata":{"id":"wGuAUjrHssf7","execution":{"iopub.status.busy":"2024-09-02T18:57:42.478930Z","iopub.status.idle":"2024-09-02T18:57:42.479450Z","shell.execute_reply.started":"2024-09-02T18:57:42.479187Z","shell.execute_reply":"2024-09-02T18:57:42.479213Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train_seqs = pad_sequence([torch.tensor(seq).to(device) for seq in x_train_seqs], batch_first=True, padding_value=2)\ny_train_seqs = pad_sequence([torch.tensor(seq).to(device) for seq in y_train_seqs], batch_first=True, padding_value=3)\n\nx_test_seqs = pad_sequence([torch.tensor(seq).to(device) for seq in x_test_seqs], batch_first=True, padding_value=2)\ny_test_seqs = pad_sequence([torch.tensor(seq).to(device) for seq in y_test_seqs], batch_first=True, padding_value=3)\n\ntrain_dataset = TranslationDataset(x_train_seqs, y_train_seqs)\ntest_dataset = TranslationDataset(x_test_seqs, y_test_seqs)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)","metadata":{"id":"janu6tFVsuYO","execution":{"iopub.status.busy":"2024-09-02T18:57:42.480844Z","iopub.status.idle":"2024-09-02T18:57:42.481350Z","shell.execute_reply.started":"2024-09-02T18:57:42.481087Z","shell.execute_reply":"2024-09-02T18:57:42.481114Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_dim = len(en_vocab)  # Vocabulary size for input\noutput_dim = len(fr_vocab)  # Vocabulary size for output\nd_model = 256\nnhead = 8\nnum_encoder_layers = 4\nnum_decoder_layers = 4\nmax_len = max(len(sentence) for sentence in x_train)\nbatch_size = 128\nnum_epochs = 25\ndropout_val = 0.1\nsrc_pad_index = 2\ntgt_pad_index = 3","metadata":{"id":"3Ne-ViBOaE-_","execution":{"iopub.status.busy":"2024-09-02T18:57:42.482442Z","iopub.status.idle":"2024-09-02T18:57:42.482957Z","shell.execute_reply.started":"2024-09-02T18:57:42.482680Z","shell.execute_reply":"2024-09-02T18:57:42.482719Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_len","metadata":{"id":"GMP4bxiIcYie","execution":{"iopub.status.busy":"2024-09-02T18:57:42.484102Z","iopub.status.idle":"2024-09-02T18:57:42.484609Z","shell.execute_reply.started":"2024-09-02T18:57:42.484345Z","shell.execute_reply":"2024-09-02T18:57:42.484372Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len):\n        super(PositionalEncoding, self).__init__()\n        pe = torch.zeros(max_len, d_model).to(device)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1).to(device)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)).to(device)\n        pe[:, 0::2] = torch.sin(position * div_term).to(device)\n        pe[:, 1::2] = torch.cos(position * div_term).to(device)\n        pe = pe.unsqueeze(0).transpose(0, 1).to(device)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :x.size(1), :]\n        return x\n","metadata":{"id":"wxjC1aR8F7Lu","execution":{"iopub.status.busy":"2024-09-02T18:57:42.486179Z","iopub.status.idle":"2024-09-02T18:57:42.486692Z","shell.execute_reply.started":"2024-09-02T18:57:42.486433Z","shell.execute_reply":"2024-09-02T18:57:42.486460Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, input_dim, model_dim, num_heads, num_encoder_layers, num_decoder_layers, output_dim, max_len, dropout_val):\n        super(Transformer, self).__init__()\n        self.model_dim = model_dim\n        self.positional_encoding = PositionalEncoding(d_model=model_dim, max_len=max_len)\n        self.transformer = nn.Transformer(d_model=model_dim, nhead=num_heads, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers, dropout=dropout_val, batch_first=True)\n        self.input_embedding = nn.Embedding(input_dim, model_dim, padding_idx=src_pad_index)\n        self.output_embedding = nn.Embedding(output_dim, model_dim, padding_idx=tgt_pad_index)\n        self.layer_norm_src = nn.LayerNorm(d_model)\n        self.layer_norm_tgt = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout_val)\n        self.fc_out = nn.Linear(model_dim, output_dim)\n        \n    def create_look_ahead_mask(self, size):\n        mask = torch.triu(torch.ones(size, size, dtype=torch.float), diagonal=1)\n        return mask.masked_fill(mask == 1, float('-inf'))  # Shape: [seq_len, seq_len]\n\n    def forward(self, src, tgt, src_padding_mask=None, tgt_padding_mask=None):\n        src = (self.input_embedding(src) * math.sqrt(self.model_dim)).to(device)\n        tgt = (self.output_embedding(tgt) * math.sqrt(self.model_dim)).to(device)\n        \n        src = self.dropout(src).to(device)\n        tgt = self.dropout(tgt).to(device)\n        \n        src = self.layer_norm_src(src)\n        tgt = self.layer_norm_tgt(tgt)\n#         print(\"done till here\")\n        src = self.positional_encoding(src).to(device)\n        tgt = self.positional_encoding(tgt).to(device)\n        tgt_look_ahead_mask = self.create_look_ahead_mask(tgt.size(1))  # Shape: [tgt_seq_len, tgt_seq_len]\n        tgt_look_ahead_mask = tgt_look_ahead_mask.to(device)\n#         print(\"done till here\")\n#         print(src.size(), tgt.size())\n        output = self.transformer(src, tgt, \n                                  src_key_padding_mask=src_padding_mask.to(device), \n                                  tgt_key_padding_mask=tgt_padding_mask.to(device), \n                                  tgt_mask=tgt_look_ahead_mask).to(device)\n        return self.fc_out(output).to(device)","metadata":{"id":"8fqZC2sovQTc","execution":{"iopub.status.busy":"2024-09-02T18:57:42.489492Z","iopub.status.idle":"2024-09-02T18:57:42.489952Z","shell.execute_reply.started":"2024-09-02T18:57:42.489709Z","shell.execute_reply":"2024-09-02T18:57:42.489734Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# input_dim = 10  # Vocabulary size\n# model_dim = 512  # Dimension of model\n# num_heads = 8\n# num_layers = 6\n# output_dim = 10  # Output dimension\n\nmodel = Transformer(input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, output_dim, max_len, dropout_val).to(device)","metadata":{"id":"2otD6n4qEpLK","execution":{"iopub.status.busy":"2024-09-02T18:57:42.492149Z","iopub.status.idle":"2024-09-02T18:57:42.492959Z","shell.execute_reply.started":"2024-09-02T18:57:42.492682Z","shell.execute_reply":"2024-09-02T18:57:42.492711Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(ignore_index = tgt_pad_index)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"id":"GjjKvtcNE4gY","execution":{"iopub.status.busy":"2024-09-02T18:57:42.494501Z","iopub.status.idle":"2024-09-02T18:57:42.495380Z","shell.execute_reply.started":"2024-09-02T18:57:42.495099Z","shell.execute_reply":"2024-09-02T18:57:42.495128Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_model(model, epoch, directory='/kaggle/working/'):\n    filename = (f\"ep-{epoch+1}-.pth\")\n    filepath = os.path.join(directory, filename)\n    \n    torch.save(model.state_dict(), filepath)\n    print(f\"Model saved to {filepath}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-02T18:57:42.496725Z","iopub.status.idle":"2024-09-02T18:57:42.497200Z","shell.execute_reply.started":"2024-09-02T18:57:42.496942Z","shell.execute_reply":"2024-09-02T18:57:42.496967Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    model.train()\n\n    losses = 0.0\n    correct_predictions = 0\n    total_predictions = 0\n\n    for src_batch, tgt_batch in tqdm(train_loader):\n        optimizer.zero_grad()\n        \n        src_batch = src_batch.to(device)\n        tgt_batch = tgt_batch.to(device)\n\n        src_padding_mask = ((src_batch == 2).bool()).to(device)\n        tgt_padding_mask = (tgt_batch == 3).bool().to(device)\n\n        output = model(src_batch, tgt_batch, src_padding_mask, tgt_padding_mask).to(device)\n\n        loss = criterion(output.view(-1, output.size(-1)), tgt_batch.contiguous().view(-1))\n        losses += loss.item()  # Use .item() to get the scalar value\n\n        loss.backward()\n        optimizer.step()\n        \n        preds = output.argmax(dim=-1)  \n        correct_predictions += (preds == tgt_batch).sum().item()\n        total_predictions += tgt_batch.numel()\n    \n    avg_epoch_loss = losses / len(train_loader)\n    accuracy = correct_predictions / total_predictions\n\n    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_epoch_loss}, Accuracy: {accuracy}')\n    save_model(model, epoch)\n    print(f'Model saved at {epoch+1}')","metadata":{"id":"bSstABZ2E8a3","execution":{"iopub.status.busy":"2024-09-02T18:57:42.498773Z","iopub.status.idle":"2024-09-02T18:57:42.499260Z","shell.execute_reply.started":"2024-09-02T18:57:42.498993Z","shell.execute_reply":"2024-09-02T18:57:42.499018Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\n\n# Initialize metrics\ntotal_loss = 0.0\ncorrect_predictions = 0\ntotal_predictions = 0\n\n# Disable gradient calculations for evaluation\nwith torch.no_grad():\n    for src_batch, tgt_batch in tqdm(test_loader):\n        src_batch = src_batch.to(device)\n        tgt_batch = tgt_batch.to(device)\n\n        src_padding_mask = (src_batch == 0).to(dtype=torch.bool, device=device)\n        tgt_padding_mask = (tgt_batch == 0).to(dtype=torch.bool, device=device)\n\n        # Forward pass\n        output = model(src_batch, tgt_batch, src_padding_mask, tgt_padding_mask)\n\n        # Compute loss\n        loss = criterion(output.view(-1, output.size(-1)), tgt_batch.contiguous().view(-1))\n        total_loss += loss.item()  # Use .item() to get the scalar value\n\n        # Compute predictions\n        preds = output.argmax(dim=-1)\n\n        # Calculate correct predictions\n        correct_predictions += (preds == tgt_batch).sum().item()\n        total_predictions += tgt_batch.numel()\n\n# Calculate average loss and accuracy\navg_loss = total_loss / len(test_loader)\naccuracy = correct_predictions / total_predictions\n\n# Print evaluation results\nprint(f'Loss: {avg_loss}, Accuracy: {accuracy}')","metadata":{"id":"0HvmcAu3q3lm","execution":{"iopub.status.busy":"2024-09-02T18:57:42.500409Z","iopub.status.idle":"2024-09-02T18:57:42.500905Z","shell.execute_reply.started":"2024-09-02T18:57:42.500647Z","shell.execute_reply":"2024-09-02T18:57:42.500673Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"en_vocab[0]","metadata":{"id":"-g52dRkdmuGH","execution":{"iopub.status.busy":"2024-09-02T18:57:42.502806Z","iopub.status.idle":"2024-09-02T18:57:42.503349Z","shell.execute_reply.started":"2024-09-02T18:57:42.503058Z","shell.execute_reply":"2024-09-02T18:57:42.503103Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"HcYy35MBsZ1q"},"outputs":[],"execution_count":null}]}